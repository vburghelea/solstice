Below are the issues I see in the provided subset (line numbers are based on the pasted file contents and may be off by a couple lines if the source differs).

---

## 1) Audit log immutability not enforced at DB level (in provided code)

1. **System**: Audit
2. **Severity**: High
3. **Location**: `src/db/schema/audit.schema.ts:6-41`
4. **Issue**: The schema defines the `audit_logs` table but there is no DB-level immutability enforcement shown (e.g., `BEFORE UPDATE OR DELETE` trigger, revoking UPDATE/DELETE privileges). If this exists in migrations outside the subset, great—but in the code shown, immutability relies on convention.
5. **Compliance Impact**: Breaks “append-only/immutable audit” expectations (SEC-AGG-004) and weakens PIPEDA accountability/auditability (tamper resistance).
6. **Fix**: Add DB trigger(s) and permissions:
   - Trigger to block UPDATE/DELETE.
   - Ensure the application DB role cannot UPDATE/DELETE on `audit_logs`.
   - Consider separating an “audit_writer” role and a “read_only_audit” role.

---

## 2) Hash chain can break due to clock skew + ordering by `occurredAt`

1. **System**: Audit
2. **Severity**: High
3. **Location**: `src/lib/audit/index.ts:166-170, 193-195, 247-268`
4. **Issue**: The chain “previous entry” lookup and verification both order by `occurredAt`, but inserts set `occurredAt: new Date()` (application time). In distributed runtimes (Lambda/ECS), clock skew or out-of-order timestamps can produce rows whose `occurredAt` is earlier than already-written rows. Verification then re-orders them and the chain fails even without tampering.
5. **Compliance Impact**: Undermines “proper hash chain verification” (SEC-AGG-004). You’ll get false integrity failures and lose confidence in audit evidence.
6. **Fix**:
   - Use DB time for ordering and hashing (e.g., omit `occurredAt` on insert and rely on `defaultNow()`), or create a monotonic sequence/ULID.
   - Verify chain in the same order it’s built (prefer `createdAt` from DB over application-set timestamps).
   - Add a deterministic secondary sort (e.g., `ORDER BY created_at, id`) and use the same for both “prev” selection and verify.

---

## 3) Hash chain can fork under concurrency (non-atomic prevHash selection)

1. **System**: Audit
2. **Severity**: High
3. **Location**: `src/lib/audit/index.ts:166-170, 193-209`
4. **Issue**: `logAuditEntry()` does: (1) SELECT latest hash, (2) INSERT new row. Without a lock/transactional serialization, concurrent writers can both read the same “latest” and insert two rows with the same `prevHash`, forking the chain and causing verification failures.
5. **Compliance Impact**: Weak tamper-evidence; integrity verification becomes unreliable under normal load (SEC-AGG-004).
6. **Fix**:
   - Serialize chain writes using `pg_advisory_xact_lock(...)` inside a transaction, or
   - Maintain a “chain head” row/table updated with `SELECT … FOR UPDATE`, or
   - Build independent chains (e.g., per `targetOrgId`) with per-chain locking.

---

## 4) Hash payload omits `occurredAt`/`createdAt` (timestamp tampering not detectable)

1. **System**: Audit
2. **Severity**: Medium
3. **Location**: `src/lib/audit/index.ts:175-189, 273-287`
4. **Issue**: The `entryHash` is computed from payload fields that **do not include** `occurredAt` (or `createdAt`). If a privileged DB actor could alter timestamps (or if immutability isn’t perfectly enforced), the hash chain still verifies because verification ignores those timestamps too.
5. **Compliance Impact**: Reduced tamper-evidence strength for forensic timelines (accountability/audit evidentiary quality).
6. **Fix**: Include `occurredAt` (and ideally `id`) in the hashed payload. If you move to DB-time, hash the DB timestamp value.

---

## 5) `actorIp` may be invalid for `inet` column (x-forwarded-for parsing)

1. **System**: Audit
2. **Severity**: Medium
3. **Location**: `src/lib/audit/index.ts:142-143, 197-198` + schema `src/db/schema/audit.schema.ts:14-15`
4. **Issue**: `actorIp` is read directly from `x-forwarded-for`, which often contains a comma-separated list (`client, proxy1, proxy2`). That will not fit Postgres `inet` and can cause audit insert failures (i.e., you lose audit events).
5. **Compliance Impact**: Audit completeness risk (PIPEDA accountability; SEC-AGG-004 completeness).
6. **Fix**: Parse the first IP, trim, validate, and only then write to `inet`. Otherwise store null (or switch column to text if you truly want raw header).

---

## 6) PII hashing/redaction is shallow; nested objects can leak sensitive fields

1. **System**: Audit
2. **Severity**: High
3. **Location**: `src/lib/audit/index.ts:24-34, 107-128, 84-105`
4. **Issue**: `createAuditDiff()` diffs at the **top-level key** only. If an object field changes (e.g., `emergencyContact`), the diff stores the **entire object**, but the hashing rules expect dotted paths like `emergencyContact.phone`. Those dotted-path rules never fire when the change key is just `emergencyContact`. Same for nested secrets/tokens inside objects.
5. **Compliance Impact**: Violates the documented “field-level diffs with redacted/hashes” intent and can place unredacted PII into immutable audit logs (PIPEDA safeguards + limiting collection; audit policy section “PII Handling”).
6. **Fix**:
   - Implement a **deep diff** that produces dotted paths (`emergencyContact.phone`) rather than whole-object diffs.
   - Or recursively sanitize objects (walk all keys and apply redaction/hash rules by path).
   - Add tests for nested objects and ensure the output never contains raw phone/DOB/emergency contact phone.

---

## 7) Metadata is not sanitized before hashing/storing

1. **System**: Audit
2. **Severity**: Medium
3. **Location**: `src/lib/audit/index.ts:175-189`
4. **Issue**: `metadata: input.metadata ?? {}` is stored and hashed as-is. Callers can accidentally log secrets, tokens, emails, or other PII in metadata and it will become immutable.
5. **Compliance Impact**: Increases risk of unnecessary PII retention and disclosure (PIPEDA limiting collection/use; safeguards).
6. **Fix**: Add a `sanitizeMetadata()` step similar to `sanitizeChanges()` (at minimum redact keys matching `token`, `secret`, `password`, etc., and consider allowlisting metadata keys per action).

---

## 8) Double-sanitization can double-hash values

1. **System**: Audit
2. **Severity**: Low
3. **Location**: `src/lib/audit/index.ts:107-128` and `src/lib/audit/index.ts:172-174`
4. **Issue**: `createAuditDiff()` already calls `sanitizeChanges()`, and `logAuditEntry()` calls `sanitizeChanges()` again. If callers pass a diff from `createAuditDiff()` into `logDataChange()`, hashed fields can get hashed again.
5. **Compliance Impact**: Not a direct PIPEDA violation, but it makes audit values inconsistent and harder to reason about/debug during DSAR/audit reviews.
6. **Fix**: Decide on one sanitization point:
   - Either return **raw diff** from `createAuditDiff()` and sanitize only inside `logAuditEntry()`, or
   - Mark sanitized diffs and skip re-sanitization.

---

## 9) Audit UI date filtering likely fails schema validation

1. **System**: Audit
2. **Severity**: Medium
3. **Location**: `src/features/audit/audit.schemas.ts:16-17` and `src/features/audit/components/audit-log-table.tsx:103-112, 41-43`
4. **Issue**: `listAuditLogsSchema` requires `from`/`to` as `z.iso.datetime()`, but the UI uses `<Input type="date">` which produces `YYYY-MM-DD` (not an ISO datetime). This can cause server-side validation errors and break filtering/export in practice.
5. **Compliance Impact**: Operational audit access is impaired; admins may be unable to export/filter accurately (audit operational requirement).
6. **Fix**:
   - Change schema to accept `z.string().regex(/^\d{4}-\d{2}-\d{2}$/)` for date-only, or
   - Convert `YYYY-MM-DD` to `YYYY-MM-DDT00:00:00.000Z` (and end-of-day for `to`) before calling the server fn.

---

## 10) Missing admin authorization on privacy “admin” mutations

1. **System**: Privacy
2. **Severity**: High
3. **Location**: `src/features/privacy/privacy.mutations.ts:23-58, 133-169, 171-216`
4. **Issue**: Several server functions are gated only by `assertFeatureEnabled("sin_admin_privacy")` + “has session”, but do **not** call `requireAdmin()`:
   - `createPolicyDocument` (publishes policy docs)
   - `updatePrivacyRequest` (changes DSAR status/results)
   - `upsertRetentionPolicy` (changes retention rules)

   If the feature flag is enabled tenant-wide, any authenticated user could modify policies/DSAR/retention.

5. **Compliance Impact**: Severe breach risk (PIPEDA safeguards/accountability). DSAR records and retention controls must be admin-restricted.
6. **Fix**: Add `requireAdmin(sessionUser.id)` (or a stricter “privacy officer” role) to each of these handlers. Also consider server-side auditing of these actions as ADMIN category.

---

## 11) `getLatestPolicyDocument` can return unpublished/future-effective policies

1. **System**: Privacy
2. **Severity**: Medium
3. **Location**: `src/features/privacy/privacy.queries.ts:25-42`
4. **Issue**: The query orders by `effectiveDate DESC` but does not filter for `publishedAt IS NOT NULL` and does not ensure `effectiveDate <= today`. Users could be shown (and accept) a policy that is not yet in effect or not officially published.
5. **Compliance Impact**: Consent validity risk (PIPEDA consent/awareness expectations).
6. **Fix**: Filter to published policies and effective ones, e.g.:
   - `where(and(eq(type, data), isNotNull(publishedAt), lte(effectiveDate, today)))`
   - Or define “latest” by `publishedAt`.

---

## 12) DSAR export likely includes secrets/tokens from auth tables

1. **System**: Privacy
2. **Severity**: High
3. **Location**: `src/features/privacy/privacy.mutations.ts:228-411` (notably `accounts`, `sessions`, `twoFactor`, `verifications` at `371-410`)
4. **Issue**: The export payload includes full rows from `account`, `session`, `twoFactor`, and `verification`. Those tables often contain highly sensitive material (session tokens, OAuth refresh tokens, 2FA secrets/backup codes, verification codes/hashes). DSAR access/export should not disclose credentials/secrets.
5. **Compliance Impact**: Safeguards violation risk (PIPEDA Principle 4.7). Also increases account takeover risk.
6. **Fix**:
   - Create explicit **export DTOs** (allowlist fields) for each table.
   - Redact/omit any token/secret fields (and any hashes that function as credentials).
   - Consider excluding sessions entirely, or include only metadata (createdAt, lastSeenAt, device info) if needed.

---

## 13) DSAR handlers don’t validate request type/status before acting

1. **System**: Privacy
2. **Severity**: High
3. **Location**: `src/features/privacy/privacy.mutations.ts:255-270` and `492-510`
4. **Issue**: `generatePrivacyExport()` and `applyPrivacyErasure()` do not check `request.type` (export/access vs erasure) and do not enforce status transitions (e.g., prevent re-processing completed requests). An admin UI bug or direct call could apply erasure to the wrong request.
5. **Compliance Impact**: Improper DSAR processing (PIPEDA individual access/accuracy; potentially destructive actions).
6. **Fix**:
   - In `generatePrivacyExport`: require `request.type in ('access','export')` and `status in ('pending','processing')`.
   - In `applyPrivacyErasure`: require `request.type === 'erasure'`.
   - Enforce state machine transitions server-side.

---

## 14) DSAR “correction” request type exists but no workflow to fulfill it

1. **System**: Privacy
2. **Severity**: Medium
3. **Location**: `src/db/schema/privacy.schema.ts:20-25`, `src/features/privacy/components/privacy-dashboard.tsx:31-45`, `src/features/privacy/components/privacy-admin-panel.tsx` (no correction action)
4. **Issue**: Users can submit a `correction` request, but there’s no dedicated processing path beyond changing status/notes. There’s no mechanism to capture “what is wrong / what is the correct value” and apply it with audit trail.
5. **Compliance Impact**: Incomplete DSAR workflow (PIPEDA accuracy and individual access/correction expectations).
6. **Fix**:
   - Extend `privacy_requests` with a `details` JSON field (or separate `privacy_request_corrections` table) to store requested corrections.
   - Add admin UI to review/apply corrections.
   - Apply changes with proper audit logging (diffs, redaction rules).

---

## 15) DSAR export stored to S3 without explicit encryption/retention controls

1. **System**: Privacy
2. **Severity**: Medium
3. **Location**: `src/features/privacy/privacy.mutations.ts:413-428`
4. **Issue**: `PutObjectCommand` writes highly sensitive exports but does not explicitly set SSE-KMS, object tags (retention), or object lock settings. You might rely on bucket policy defaults, but the code gives no guarantee.
5. **Compliance Impact**: Safeguards + retention control risk (PIPEDA Principle 4.7 and 4.5).
6. **Fix**:
   - Explicitly set `ServerSideEncryption: "aws:kms"` and `SSEKMSKeyId` (if not enforced by bucket policy).
   - Add lifecycle tags/metadata to support retention/purge.
   - Consider writing to a bucket/prefix with Object Lock + restricted access.

---

## 16) DSAR download endpoint doesn’t enforce “completed” state or log access

1. **System**: Privacy
2. **Severity**: Medium
3. **Location**: `src/features/privacy/privacy.queries.ts:115-156`
4. **Issue**: `getPrivacyExportDownloadUrl` will sign any `resultUrl` if present, without checking `status === 'completed'`. It also does not audit the download action (who accessed what export).
5. **Compliance Impact**: Weak accountability around DSAR disclosure (PIPEDA accountability + access logging expectations).
6. **Fix**:
   - Require `request.status === 'completed'`.
   - Add an audit entry (e.g., `EXPORT.DSAR_DOWNLOAD`) with actor and requestId.
   - Consider step-up auth for admins before issuing the pre-signed URL.

---

## 17) DSAR erasure does not remove previously generated DSAR export artifacts

1. **System**: Privacy
2. **Severity**: High
3. **Location**: `src/features/privacy/privacy.mutations.ts:500-659`
4. **Issue**: `applyPrivacyErasure` deletes submission files and anonymizes DB records, but it does not delete DSAR export objects previously written to `privacy/exports/...` (or other “artifact” objects tied to the user). Those exports contain full PII snapshots.
5. **Compliance Impact**: “Erasure” is incomplete (PIPEDA limiting retention/use). You may retain a full PII export even after erasure.
6. **Fix**:
   - Enumerate and delete `privacy/exports/${userId}/` objects (and any other user-scoped artifacts).
   - Or apply strict lifecycle rules so DSAR exports auto-expire quickly (e.g., 7–30 days).
   - Record deletions in audit log.

---

## 18) DSAR erasure leaves some user-linked preference data behind

1. **System**: Privacy
2. **Severity**: Medium
3. **Location**: `src/features/privacy/privacy.mutations.ts:468-577`
4. **Issue**: `applyPrivacyErasure` deletes `notifications` but does not delete `notificationPreferences` (and potentially other user-linked “profile” tables not shown here). If you treat preferences as personal information, this is incomplete cleanup.
5. **Compliance Impact**: Retains personal data beyond necessity after erasure request (PIPEDA retention minimization).
6. **Fix**: Delete or reset `notificationPreferences` for the user (and review other user-linked tables for the same pattern).

---

## 19) File erasure path can drop DB rows even if S3 deletion fails

1. **System**: Privacy
2. **Severity**: Medium
3. **Location**: `src/features/privacy/privacy.mutations.ts:541-547, 613-620`
4. **Issue**: The code deletes S3 objects first and then deletes DB rows. But it deletes DB rows regardless of whether S3 deletion succeeded fully (it tracks attempted/deleted, but doesn’t branch behavior). That can leave orphaned objects in S3 with no DB pointer, making later cleanup harder.
5. **Compliance Impact**: PII may remain in object storage after erasure; accountability/cleanup becomes weaker (PIPEDA safeguards + retention).
6. **Fix**:
   - If `deleted < attempted`, either (a) do not delete DB rows for failed deletes, or (b) mark rows as “pending_delete” and retry via job.
   - Add a periodic reconciliation job to delete orphaned keys.

---

## 20) Retention policies exist, but enforcement/archival/purge is not present in the subset

1. **System**: Privacy
2. **Severity**: High
3. **Location**: `src/db/schema/privacy.schema.ts:89-105`, `src/features/privacy/privacy.mutations.ts:171-216`
4. **Issue**: You can configure retention policies, but I don’t see any server-only job that enforces retention windows, archives audit logs to S3 Glacier with Object Lock, or purges expired records/objects. The docs explicitly require these procedures (`audit-retention-policy.md`).
5. **Compliance Impact**: Violates “limiting retention” and documented retention controls (PIPEDA Principle 4.5), plus your own audit retention policy commitments.
6. **Fix**:
   - Implement a scheduled job (EventBridge/cron) that:
     - Archives data types that require it (audit logs → S3 Glacier + Object Lock).
     - Purges only when retention expires and no legal hold exists.
     - Records purge/archive actions in audit logs.

   - Implement S3 lifecycle/object-lock policies and verify with tests.

---

## 21) “Legal hold” capability is not modeled as required (only a boolean on policy)

1. **System**: Privacy
2. **Severity**: Medium
3. **Location**: `src/db/schema/privacy.schema.ts:97`
4. **Issue**: `legalHold` is a boolean on `retention_policies`, which is a global toggle per data type. The policy document requires legal holds at record/user/org level (“Held records are exempt from purge until released”). That’s not represented here.
5. **Compliance Impact**: Inability to comply with legal hold requirements in retention policy; risk of improper deletion during litigation/investigation.
6. **Fix**: Add a `legal_holds` table keyed by `{scope_type, scope_id, reason, applied_by, applied_at, released_at}` and have retention enforcement consult it.

---

## 22) Any authenticated user can create arbitrary notifications (and spoof audit actor)

1. **System**: Notifications
2. **Severity**: High
3. **Location**: `src/features/notifications/notifications.mutations.ts:122-157`
4. **Issue**: `createNotification` does not require session auth at all and accepts `data.userId`. If exposed to the client, a malicious user could create/spam notifications to other users (and it logs audit with `actorUserId: data.userId`, which is doubly wrong).
5. **Compliance Impact**: Security + integrity issue (PIPEDA safeguards; audit integrity). Could be used for phishing/social engineering via in-app notifications.
6. **Fix**:
   - Make this a **server-only** function (`createServerOnlyFn`) for internal use, OR
   - Require authenticated session and enforce `data.userId === sessionUser.id` unless admin.
   - Fix audit logging to record the real actor.

---

## 23) Missing admin checks for notification admin endpoints

1. **System**: Notifications
2. **Severity**: High
3. **Location**: `src/features/notifications/notifications.mutations.ts:159-193, 248-254`
4. **Issue**: `createNotificationTemplate` and `scheduleNotification` are only feature-gated and require “any session user”, but do not call `requireAdmin()`. `update/delete` do require admin, so `create/schedule` are inconsistent.
5. **Compliance Impact**: Unauthorized users could create templates or schedule notifications at scale (security + auditability).
6. **Fix**: Add `requireAdmin(userId)` (or a specific notifications admin role) to both handlers.

---

## 24) Notification dispatch/digest audit entries attribute the actor incorrectly

1. **System**: Notifications
2. **Severity**: Medium
3. **Location**:
   - `src/lib/notifications/send.ts:225-241`
   - `src/lib/notifications/digest.ts:108-118`

4. **Issue**: Both `NOTIFICATION_DISPATCH` and `NOTIFICATION_DIGEST_SENT` audit entries set `actorUserId` to the **recipient** (`payload.userId` / `preference.userId`). That makes the audit log claim the user performed the action.
5. **Compliance Impact**: Audit records become misleading (PIPEDA accountability; SEC-AGG-004 correctness).
6. **Fix**:
   - Use `actorUserId: null` (system) or pass the initiating actor through `NotificationDispatch` (e.g., `initiatorUserId`).
   - Put the recipient in `targetId`/metadata, not as actor.

---

## 25) Email idempotency breaks when in-app is disabled (duplicate email risk)

1. **System**: Notifications
2. **Severity**: Medium
3. **Location**: `src/lib/notifications/send.ts:143-174, 199-211`
4. **Issue**: Email idempotency relies on `notifications.metadata.emailSentAt`, but that metadata only exists if `allowInApp` is true (because only then do you insert/select/update a notification row). If a user disables in-app but allows email immediate, retries can re-send duplicate emails.
5. **Compliance Impact**: Operational + user trust issue; can become a “notification spam” problem (and may raise privacy concerns if sensitive alerts are duplicated).
6. **Fix**:
   - Persist a separate idempotency record keyed by `notificationId` (even when in-app is disabled), or
   - Always write a minimal “dispatch record” row even if in-app UI is off, solely to track `emailSentAt`.

---

## 26) Scheduled notifications don’t support org/role broadcasts (schema mismatch)

1. **System**: Notifications
2. **Severity**: Medium
3. **Location**: `src/lib/notifications/scheduler.ts:72-74`
4. **Issue**: The schema supports `organizationId` and `roleFilter`, but `processScheduledNotifications` throws if `job.userId` is null (“not supported yet”). This is a functional gap vs the documented design.
5. **Compliance Impact**: Reporting reminders/security alerts may not reach intended audiences; could impact operational compliance timelines (e.g., missed reminders).
6. **Fix**:
   - Implement broadcast resolution: find users by `organizationMembers` + `roleFilter` and enqueue per-user with stable `notificationId`s.
   - Update retry/error handling to handle partial sends safely.

---

## 27) SendGrid email path still exists (data residency / sub-processor risk)

1. **System**: Notifications (Email)
2. **Severity**: Medium
3. **Location**: `src/lib/email/sendgrid.ts:84-163`
4. **Issue**: The code can send real email via SendGrid when `SENDGRID_API_KEY` is set. Your backlog/docs emphasize AWS SES in `ca-central-1` for residency. If SendGrid is enabled in production (even accidentally), it can violate your stated residency posture for email content.
5. **Compliance Impact**: Cross-border processing risk; PIPEDA accountability for third-party processing + your stated “Canada-only” operational requirement.
6. **Fix**:
   - Hard-disable SendGrid in production builds/environments (fail closed).
   - Prefer SES-only for SIN production, and remove/guard SendGrid paths behind explicit non-prod flags.

---

If you want, I can also propose a concrete “patch plan” (what to change first to de-risk compliance fastest), but the above list is the actionable set of findings tied directly to your key questions.
